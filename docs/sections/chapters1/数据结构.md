# 数据结构

## 一、数据结构概述

数据结构是计算机科学的一个重要概念，它指的是在计算机中组织和存储数据的特殊方式，使得我们可以以不同的方式有效地访问和修改这些数据。数据结构通常包括数组、链表、树、哈希表、堆、队列、栈等。

数据结构的重要性主要表现在以下几个方面：

1. 高效性：有效的数据结构可以使我们以更快的速度检索和更新数据。例如，如果我们将数据存储在一个经过排序的数组或平衡搜索树中，我们可以快速地查找特定的元素。
2. 可扩展性：一些数据结构，如哈希表，可以随着数据的增长而有效地扩展。这使得我们可以在不牺牲太多性能的情况下处理更大的数据集。
3. 抽象性：数据结构通常隐藏了数据的实际表示方式，只提供一些基本的操作（如添加、删除和查找元素）。这使得程序员可以更专注于解决问题，而不必关心数据的具体组织方式。
4. 提升算法效率：合适的数据结构能够显著提高算法的运行效率。例如，二叉搜索树提供了高效的搜索、插入和删除操作，哈希表提供了近乎常数时间的搜索、插入和删除操作。
5. 缩小资源使用：合适的数据结构可以减少对计算机内存和存储资源的使用。

因此，选择和使用正确的数据结构是计算机编程中的关键任务之一。在设计和实现程序时，理解和掌握各种数据结构的工作原理和适用场景是非常重要的。

## 二、算法与数据结构的区别

算法和数据结构是计算机科学中两个核心概念，虽然它们密切相关，但它们之间有着明显的区别。

**数据结构**是一种组织和存储数据的方式，它定义了数据的存储方式和数据之间的关系，以及对数据进行的各种操作。数据结构为数据提供了一种组织方式，以及在该组织下如何进行操作的一种方法。常见的数据结构包括数组、栈、队列、链表、树、图、哈希表等。

**算法**则是解决特定问题的一系列步骤或规则。它涉及到如何操作数据，以实现特定的功能或达到特定的结果。算法定义了应该执行哪些步骤，以及应该如何执行这些步骤以解决特定的问题。常见的算法包括排序算法（如冒泡排序、快速排序）、查找算法（如二分查找）、图算法（如深度优先搜索、广度优先搜索）等。

**程序 = 数据结构 + 算法**

![数据结构与算法的关系](https://raw.githubusercontent.com/sanmaomashi/Salute_DataStructure/main/img/4.png)

简单来说，数据结构是对数据的存储，而算法是对数据的处理。在编程中，我们通常会将算法应用到数据结构上，以解决实际问题。对数据结构和算法的深入理解和熟练掌握，对于编写高效的代码和构建高效的程序至关重要。

## 三、数据结构分类

数据结构是一种在计算机中存储和组织数据的方式。不同的数据结构有不同的优势和用途。数据结构可以从逻辑结构和物理结构两个维度进行分类。

### 1. 逻辑结构

**「逻辑结构」揭示了数据元素之间的逻辑关系**。在数组和链表中，数据按照顺序依次排列，体现了数据之间的线性关系；而在树中，数据从顶部向下按层次排列，表现出祖先与后代之间的派生关系；图则由节点和边构成，反映了复杂的网络关系。

逻辑结构通常分为「线性」和「非线性」两类。线性结构比较直观，指数据在逻辑关系上呈线性排列；非线性结构则相反，呈非线性排列，例如网状或树状结构。

![线性与非线性数据结构](https://raw.githubusercontent.com/sanmaomashi/Salute_DataStructure/main/img/19.png)

#### 1.1 线性数据结构

线性数据结构是元素构成序列或列表的结构。下面是一些常见的线性数据结构：

1. **数组（Array）**：数组是一种在连续内存位置存储相同类型的元素的数据结构。数组的优点是可以通过索引快速访问元素，但是它的大小在创建时确定，不能动态更改。
2. **链表（Linked List）**：链表是由一组节点组成，每个节点包含元素和指向下一个节点的指针。链表的优点是可以进行高效的插入和删除操作，但是访问链表中的元素通常需要从头开始遍历。
3. **栈（Stack）**：栈是一种只允许在一端（称为“顶部”）进行插入和删除操作的特殊列表。栈遵循 LIFO（后进先出）原则。
4. **队列（Queue）**：队列是一种允许在一端添加元素，在另一端删除元素的特殊列表。队列遵循 FIFO（先进先出）原则。
5. **双端队列（Deque）**：双端队列是一种允许在两端添加和删除元素的数据结构。

#### 1.2 非线性数据结构

非线性数据结构是元素构成多级结构（例如树和图）的结构。以下是一些常见的非线性数据结构：

1. **树（Tree）**：树是由一组节点组成，其中一个节点是根，其他节点可以分为若干个不相交的子树。
2. **二叉树（Binary Tree）**：二叉树是每个节点最多有两个子节点的树：左子节点和右子节点。
3. **堆（Heap）**：堆是一种特殊的完全二叉树，满足堆属性（通常是所有父节点都大于或等于其子节点的最大堆，或所有父节点都小于或等于其子节点的最小堆）。
4. **图（Graph）**：图由一组节点（或顶点）和连接这些节点的边组成。
5. **散列表（Hashing）**：散列表是一种使用哈希函数将键映射到桶的数据结构，用于实现快速查找。

### 2. 物理结构：连续与离散

在计算机中，内存和硬盘是两种主要的存储硬件设备。「硬盘」主要用于长期存储数据，容量较大（通常可达到 TB 级别）、速度较慢。「内存」用于运行程序时暂存数据，速度较快，但容量较小（通常为 GB 级别）。

**在算法运行过程中，相关数据都存储在内存中**。下图展示了一个计算机内存条，其中每个黑色方块都包含一块内存空间。我们可以将内存想象成一个巨大的 Excel 表格，其中每个单元格都可以存储 1 byte 的数据，在算法运行时，所有数据都被存储在这些单元格中。

**系统通过「内存地址 Memory Location」来访问目标内存位置的数据**。计算机根据特定规则为表格中的每个单元格分配编号，确保每个内存空间都有唯一的内存地址。有了这些地址，程序便可以访问内存中的数据。

![内存条、内存空间、内存地址](https://raw.githubusercontent.com/sanmaomashi/Salute_DataStructure/main/img/10.png)

内存是所有程序的共享资源，当内存被某个程序占用时，其他程序无法同时使用。**因此，在数据结构与算法的设计中，内存资源是一个重要的考虑因素**。例如，算法所占用的内存峰值不应超过系统剩余空闲内存；如果运行的程序很多并且缺少大量连续的内存空间，那么所选用的数据结构必须能够存储在离散的内存空间内。

**「物理结构」反映了数据在计算机内存中的存储方式**，可分为数组的连续空间存储和链表的离散空间存储。物理结构从底层决定了数据的访问、更新、增删等操作方法，同时在时间效率和空间效率方面呈现出互补的特点。

![连续空间存储与离散空间存储](https://raw.githubusercontent.com/sanmaomashi/Salute_DataStructure/main/img/11.png)

**所有数据结构都是基于数组、链表或二者的组合实现的**。例如，栈和队列既可以使用数组实现，也可以使用链表实现；而哈希表的实现可能同时包含数组和链表。

- **基于数组可实现**：栈、队列、哈希表、树、堆、图、矩阵、张量（维度 ≥3 的数组）等；
- **基于链表可实现**：栈、队列、哈希表、树、堆、图等；

基于数组实现的数据结构也被称为「静态数据结构」，这意味着此类数据结构在初始化后长度不可变。相对应地，基于链表实现的数据结构被称为「动态数据结构」，这类数据结构在初始化后，仍可以在程序运行过程中对其长度进行调整。

## 四、基本数据类型

谈及计算机中的数据，我们会想到文本、图片、视频、语音、3D 模型等各种形式。尽管这些数据的组织形式各异，但它们都由各种基本数据类型构成。

**基本数据类型是 CPU 可以直接进行运算的类型，在算法中直接被使用**。它包括：

- 整数类型 `byte` , `short` , `int` , `long` ；
- 浮点数类型 `float` , `double` ，用于表示小数；
- 字符类型 `char` ，用于表示各种语言的字母、标点符号、甚至表情符号等；
- 布尔类型 `bool` ，用于表示“是”与“否”判断；

**所有基本数据类型都以二进制的形式存储在计算机中**。在计算机中，我们将 1 个二进制位称为 1 比特，并规定 1 字节（byte）由 8 比特（bits）组成。基本数据类型的取值范围取决于其占用的空间大小，例如：

- 整数类型 `byte` 占用 1 byte = 8 bits ，可以表示 28 个不同的数字；
- 整数类型 `int` 占用 4 bytes = 32 bits ，可以表示 232 个数字；

下表列举了各种基本数据类型的占用空间、取值范围和默认值。此表格无需硬背，大致理解即可，需要时可以通过查表来回忆。
<div class="center-table" markdown>

| 类型   | 符号     | 占用空间         | 最小值                   | 最大值                  | 默认值         |
| ------ | -------- | ---------------- | ------------------------ | ----------------------- | -------------- |
| 整数   | `byte`   | 1 byte           | $-2^7$ ($-128$)          | $2^7 - 1$ ($127$)       | $0$            |
|        | `short`  | 2 bytes          | $-2^{15}$                | $2^{15} - 1$            | $0$            |
|        | `int`    | 4 bytes          | $-2^{31}$                | $2^{31} - 1$            | $0$            |
|        | `long`   | 8 bytes          | $-2^{63}$                | $2^{63} - 1$            | $0$            |
| 浮点数 | `float`  | 4 bytes          | $1.175 \times 10^{-38}$  | $3.403 \times 10^{38}$  | $0.0 f$        |
|        | `double` | 8 bytes          | $2.225 \times 10^{-308}$ | $1.798 \times 10^{308}$ | $0.0$          |
| 字符   | `char`   | 2 bytes / 1 byte | $0$                      | $2^{16} - 1$            | $0$            |
| 布尔   | `bool`   | 1 byte / 1 bit   | $\text{false}$           | $\text{true}$           | $\text{false}$ |

</div>


那么，基本数据类型与数据结构之间有什么联系与区别呢？我们知道，数据结构是在计算机中组织与存储数据的方式。它的主语是“结构”，而非“数据”。如果想要表示“一排数字”，我们自然会想到使用数组。这是因为数组的线性结构可以表示数字的相邻关系和顺序关系，但至于存储的内容是整数 `int` 、小数 `float` 、还是字符 `char` ，则与“数据结构”无关。

换句话说，**基本数据类型提供了数据的“内容类型”，而数据结构提供了数据的“组织方式”**。

## 五、数字编码

### 1. 原码、反码和补码

从上一节的表格中我们发现，所有整数类型能够表示的负数都比正数多一个。例如，`byte` 的取值范围是 $[-128, 127]$ 。这个现象比较反直觉，它的内在原因涉及到原码、反码、补码的相关知识。在展开分析之前，我们首先给出三者的定义：

- **原码**：我们将数字的二进制表示的最高位视为符号位，其中 $0$ 表示正数，$1$ 表示负数，其余位表示数字的值。
- **反码**：正数的反码与其原码相同，负数的反码是对其原码除符号位外的所有位取反。
- **补码**：正数的补码与其原码相同，负数的补码是在其反码的基础上加 $1$ 。

![原码、反码与补码之间的相互转换](https://raw.githubusercontent.com/sanmaomashi/Salute_DataStructure/main/img/12.png)

显然，「原码」最为直观，**然而数字却是以「补码」的形式存储在计算机中的**。这是因为原码存在一些局限性。

一方面，**负数的原码不能直接用于运算**。例如，我们在原码下计算 $1 + (-2)$ ，得到的结果是 $-3$ ，这显然是不对的。

$$
\begin{aligned}
& 1 + (-2) \newline
& = 0000 \space 0001 + 1000 \space 0010 \newline
& = 1000 \space 0011 \newline
& = -3
\end{aligned}
$$

为了解决此问题，计算机引入了「反码」。例如，我们先将原码转换为反码，并在反码下计算 $1 + (-2)$ ，并将结果从反码转化回原码，则可得到正确结果 $-1$ 。

$$
\begin{aligned}
& 1 + (-2) \newline
& = 0000 \space 0001 \space \text{(原码)} + 1000 \space 0010 \space \text{(原码)} \newline
& = 0000 \space 0001 \space \text{(反码)} + 1111  \space 1101 \space \text{(反码)} \newline
& = 1111 \space 1110 \space \text{(反码)} \newline
& = 1000 \space 0001 \space \text{(原码)} \newline
& = -1
\end{aligned}
$$

另一方面，**数字零的原码有 $+0$ 和 $-0$ 两种表示方式**。这意味着数字零对应着两个不同的二进制编码，而这可能会带来歧义问题。例如，在条件判断中，如果没有区分正零和负零，可能会导致错误的判断结果。如果我们想要处理正零和负零歧义，则需要引入额外的判断操作，其可能会降低计算机的运算效率。

$$
\begin{aligned}
+0 & = 0000 \space 0000 \newline
-0 & = 1000 \space 0000
\end{aligned}
$$

与原码一样，反码也存在正负零歧义问题。为此，计算机进一步引入了「补码」。那么，补码有什么作用呢？我们先来分析一下负零的补码的计算过程：

$$
\begin{aligned}
-0 = \space & 1000 \space 0000 \space \text{(原码)} \newline
= \space & 1111 \space 1111 \space \text{(反码)} \newline
= 1 \space & 0000 \space 0000 \space \text{(补码)} \newline
\end{aligned}
$$

在负零的反码基础上加 $1$ 会产生进位，而由于 byte 的长度只有 8 位，因此溢出到第 9 位的 $1$ 会被舍弃。**从而得到负零的补码为 $0000 \space 0000$ ，与正零的补码相同**。这意味着在补码表示中只存在一个零，从而解决了正负零歧义问题。

还剩余最后一个疑惑：byte 的取值范围是 $[-128, 127]$ ，多出来的一个负数 $-128$ 是如何得到的呢？我们注意到，区间 $[-127, +127]$ 内的所有整数都有对应的原码、反码和补码，并且原码和补码之间是可以互相转换的。

然而，**补码 $1000 \space 0000$ 是一个例外，它并没有对应的原码**。根据转换方法，我们得到该补码的原码为 $0000 \space 0000$ 。这显然是矛盾的，因为该原码表示数字 $0$ ，它的补码应该是自身。计算机规定这个特殊的补码 $1000 \space 0000$ 代表 $-128$ 。实际上，$(-1) + (-127)$ 在补码下的计算结果就是 $-128$ 。

$$
\begin{aligned}
& (-127) + (-1) \newline
& = 1111 \space 1111 \space \text{(原码)} + 1000 \space 0001 \space \text{(原码)} \newline
& = 1000 \space 0000 \space \text{(反码)} + 1111  \space 1110 \space \text{(反码)} \newline
& = 1000 \space 0001 \space \text{(补码)} + 1111  \space 1111 \space \text{(补码)} \newline
& = 1000 \space 0000 \space \text{(补码)} \newline
& = -128
\end{aligned}
$$

你可能已经发现，上述的所有计算都是加法运算。这暗示着一个重要事实：**计算机内部的硬件电路主要是基于加法运算设计的**。这是因为加法运算相对于其他运算（比如乘法、除法和减法）来说，硬件实现起来更简单，更容易进行并行化处理，从而提高运算速度。

然而，这并不意味着计算机只能做加法。**通过将加法与一些基本逻辑运算结合，计算机能够实现各种其他的数学运算**。例如，计算减法 $a - b$ 可以转换为计算加法 $a + (-b)$ ；计算乘法和除法可以转换为计算多次加法或减法。

现在，我们可以总结出计算机使用补码的原因：基于补码表示，计算机可以用同样的电路和操作来处理正数和负数的加法，不需要设计特殊的硬件电路来处理减法，并且无需特别处理正负零的歧义问题。这大大简化了硬件设计，并提高了运算效率。

补码的设计非常精妙，由于篇幅关系我们先介绍到这里。建议有兴趣的读者进一步深度了解。

### 2. 浮点数编码

细心的你可能会发现：`int` 和 `float` 长度相同，都是 4 bytes，但为什么 `float` 的取值范围远大于 `int` ？这非常反直觉，因为按理说 `float` 需要表示小数，取值范围应该变小才对。

实际上，这是因为浮点数 `float` 采用了不同的表示方式。根据 IEEE 754 标准，32-bit 长度的 `float` 由以下部分构成：

- 符号位 $\mathrm{S}$ ：占 1 bit ；
- 指数位 $\mathrm{E}$ ：占 8 bits ；
- 分数位 $\mathrm{N}$ ：占 24 bits ，其中 23 位显式存储；

设 32-bit 二进制数的第 $i$ 位为 $b_i$ ，则 `float` 值的计算方法定义为：

$$
\text { val } = (-1)^{b_{31}} \times 2^{\left(b_{30} b_{29} \ldots b_{23}\right)_2-127} \times\left(1 . b_{22} b_{21} \ldots b_0\right)_2
$$

转化到十进制下的计算公式为

$$
\text { val }=(-1)^{\mathrm{S}} \times 2^{\mathrm{E} -127} \times (1 + \mathrm{N})
$$

其中各项的取值范围为

$$
\begin{aligned}
\mathrm{S} \in & \{ 0, 1\} , \quad \mathrm{E} \in \{ 1, 2, \dots, 254 \} \newline
(1 + \mathrm{N}) = & (1 + \sum_{i=1}^{23} b_{23-i} 2^{-i}) \subset [1, 2 - 2^{-23}]
\end{aligned}
$$

![IEEE 754 标准下的 float 表示方式](https://raw.githubusercontent.com/sanmaomashi/Salute_DataStructure/main/img/13.png)

以上图为例，$\mathrm{S} = 0$ ， $\mathrm{E} = 124$ ，$\mathrm{N} = 2^{-2} + 2^{-3} = 0.375$ ，易得

$$
\text { val } = (-1)^0 \times 2^{124 - 127} \times (1 + 0.375) = 0.171875
$$

现在我们可以回答最初的问题：**`float` 的表示方式包含指数位，导致其取值范围远大于 `int`** `。根据以上计算，float` 可表示的最大正数为 $2^{254 - 127} \times (2 - 2^{-23}) \approx 3.4 \times 10^{38}$ ，切换符号位便可得到最小负数。

**尽管浮点数 `float` 扩展了取值范围，但其副作用是牺牲了精度**。整数类型 `int` 将全部 32 位用于表示数字，数字是均匀分布的；而由于指数位的存在，浮点数 `float` 的数值越大，相邻两个数字之间的差值就会趋向越大。

进一步地，指数位 $E = 0$ 和 $E = 255$ 具有特殊含义，**用于表示零、无穷大、$\mathrm{NaN}$ 等**。

<div class="center-table" markdown>


| 指数位 E           | 分数位 $\mathrm{N} = 0$ | 分数位 $\mathrm{N} \ne 0$ | 计算公式                                                     |
| ------------------ | ----------------------- | ------------------------- | ------------------------------------------------------------ |
| $0$                | $\pm 0$                 | 次正规数                  | $(-1)^{\mathrm{S}} \times 2^{-126} \times (0.\mathrm{N})$    |
| $1, 2, \dots, 254$ | 正规数                  | 正规数                    | $(-1)^{\mathrm{S}} \times 2^{(\mathrm{E} -127)} \times (1.\mathrm{N})$ |
| $255$              | $\pm \infty$            | $\mathrm{NaN}$            |                                                              |

</div>

特别地，次正规数显著提升了浮点数的精度，这是因为：

- 最小正正规数为 $2^{-126} \approx 1.18 \times 10^{-38}$ ；
- 最小正次正规数为 $2^{-126} \times 2^{-23} \approx 1.4 \times 10^{-45}$ ；

双精度 `double` 也采用类似 `float` 的表示方法，此处不再详述。

## 六、字符编码

在计算机中，所有数据都是以二进制数的形式存储的，字符 `char` 也不例外。为了表示字符，我们需要建立一套「字符集」，规定每个字符和二进制数之间的一一对应关系。有了字符集之后，计算机就可以通过查表完成二进制数到字符的转换。

### 1. ASCII 字符集

「ASCII 码」是最早出现的字符集，全称为“美国标准信息交换代码”。它使用 7 位二进制数（即一个字节的低 7 位）表示一个字符，最多能够表示 128 个不同的字符。这包括英文字母的大小写、数字 0-9 、一些标点符号，以及一些控制字符（如换行符和制表符）。

![ASCII 码](https://raw.githubusercontent.com/sanmaomashi/Salute_DataStructure/main/img/14.png)

然而，**ASCII 码仅能够表示英文**。随着计算机的全球化，诞生了一种能够表示更多语言的字符集「EASCII」。它在 ASCII 的 7 位基础上扩展到 8 位，能够表示 256 个不同的字符。在世界范围内，陆续出现了一批适用于不同地区的 EASCII 字符集。这些字符集的前 128 个字符统一为 ASCII 码，后 128 个字符定义不同，以适应不同语言的需求。

### 2. GBK 字符集

后来人们发现，**EASCII 码仍然无法满足许多语言的字符数量要求**。例如，汉字大约有近十万个，光日常使用的就有几千个。中国国家标准总局于 1980 年发布了「GB2312」字符集，其收录了 6763 个汉字，基本满足了汉字的计算机处理需要。

然而，GB2312 无法处理部分的罕见字和繁体字。之后在 GB2312 的基础上，扩展得到了「GBK」字符集，它共收录了 21886 个汉字。在 GBK 编码方案中，ASCII 字符使用一个字节表示，汉字使用两个字节表示。

### 3. Unicode 字符集

随着计算机的蓬勃发展，字符集与编码标准百花齐放，而这带来了许多问题。一方面，这些字符集一般只定义了特定语言的字符，无法在多语言环境下正常工作；另一方面，同一种语言也存在多种字符集标准，如果两台电脑安装的是不同的编码标准，则在信息传递时就会出现乱码。

那个时代的人们就在想：**如果推出一个足够完整的字符集，将世界范围内的所有语言和符号都收录其中，不就可以解决跨语言环境和乱码问题了吗**？在这种想法的驱动下，一个大而全的字符集 Unicode 应运而生。

「Unicode」的全称为“统一字符编码”，理论上能容纳一百多万个字符。它致力于将全球范围内的字符纳入到统一的字符集之中，提供一种通用的字符集来处理和显示各种语言文字，减少因为编码标准不同而产生的乱码问题。

自 1991 年发布以来，Unicode 不断扩充新的语言与字符。截止 2022 年 9 月，Unicode 已经包含 149186 个字符，包括各种语言的字符、符号、甚至是表情符号等。在庞大的 Unicode 字符集中，常用的字符占用 2 字节，有些生僻的字符占 3 字节甚至 4 字节。

Unicode 是一种字符集标准，本质上是给每个字符分配一个编号（称为“码点”），**但它并没有规定在计算机中如何存储这些字符码点**。我们不禁会问：当多种长度的 Unicode 码点同时出现在同一个文本中时，系统如何解析字符？例如，给定一个长度为 2 字节的编码，系统如何确认它是一个 2 字节的字符还是两个 1 字节的字符？

对于以上问题，**一种直接的解决方案是将所有字符存储为等长的编码**。如下图所示，“Hello”中的每个字符占用 1 字节，“算法”中的每个字符占用 2 字节。我们可以通过高位填 0 ，将“Hello 算法”中的所有字符都编码为 2 字节长度。这样系统就可以每隔 2 字节解析一个字符，恢复出这个短语的内容了。

![Unicode 编码示例](https://raw.githubusercontent.com/sanmaomashi/Salute_DataStructure/main/img/15.png)

然而，ASCII 码已经向我们证明，编码英文只需要 1 字节。若采用上述方案，英文文本占用空间的大小将会是 ASCII 编码下大小的 2 倍，非常浪费内存空间。因此，我们需要一种更加高效的 Unicode 编码方法。

### 4. UTF-8 编码

目前，UTF-8 已成为国际上使用最广泛的 Unicode 编码方法。**它是一种可变长的编码**，使用 1 到 4 个字节来表示一个字符，根据字符的复杂性而变。ASCII 字符只需要 1 个字节，拉丁字母和希腊字母需要 2 个字节，常用的中文字符需要 3 个字节，其他的一些生僻字符需要 4 个字节。

UTF-8 的编码规则并不复杂，分为两种情况：

- 对于长度为 1 字节的字符，将最高位设置为 $0$ 、其余 7 位设置为 Unicode 码点。值得注意的是，ASCII 字符在 Unicode 字符集中占据了前 128 个码点。也就是说，**UTF-8 编码可以向下兼容 ASCII 码**。这意味着我们可以使用 UTF-8 来解析年代久远的 ASCII 码文本。
- 对于长度为 $n$ 字节的字符（其中 $n > 1$），将首个字节的高 $n$ 位都设置为 $1$ 、第 $n + 1$ 位设置为 $0$ ；从第二个字节开始，将每个字节的高 2 位都设置为 $10$ ；其余所有位用于填充字符的 Unicode 码点。

下图展示了“Hello算法”对应的 UTF-8 编码。将最高 $n$ 位设置为 $1$ 比较容易理解，可以向系统指出字符的长度为 $n$ 。那么，为什么要将其余所有字节的高 2 位都设置为 $10$ 呢？实际上，这个 $10$ 能够起到校验符的作用，因为在 UTF-8 编码规则下，不可能有字符的最高两位是 $10$ 。这是因为长度为 1 字节的字符的最高一位是 $0$ 。假设系统从一个错误的字节开始解析文本，字节头部的 $10$ 能够帮助系统快速的判断出异常。

![UTF-8 编码示例](https://raw.githubusercontent.com/sanmaomashi/Salute_DataStructure/main/img/16.png)

除了 UTF-8 之外，常见的编码方式还包括 UTF-16 和 UTF-32 。它们为 Unicode 字符集提供了不同的编码方法。

- **UTF-16 编码**：使用 2 或 4 个字节来表示一个字符。所有的 ASCII 字符和很多常用的非英文字符，都用 2 个字节表示；少数字符需要用到 4 个字节表示。对于 2 字节的字符，UTF-16 编码与 Unicode 码点相等。
- **UTF-32 编码**：每个字符都使用 4 个字节。这意味着 UTF-32 会比 UTF-8 和 UTF-16 更占用空间，特别是对于主要使用 ASCII 字符的文本。

从存储空间的角度看，使用 UTF-8 表示英文字符非常高效，因为它仅需 1 个字节；使用 UTF-16 编码某些非英文字符（例如中文）会更加高效，因为它只需要 2 个字节，而 UTF-8 可能需要 3 个字节。从兼容性的角度看，UTF-8 的通用性最佳，许多工具和库都优先支持 UTF-8 。

### 5. 编程语言的字符编码

对于以往的大多数编程语言，程序运行中的字符串都采用 UTF-16 或 UTF-32 这类等长的编码。这是因为在等长编码下，我们可以将字符串看作数组来处理，具体来说：

- **随机访问**: UTF-16 编码的字符串可以很容易地进行随机访问。UTF-8 是一种变长编码，要找到第 $i$ 个字符，我们需要从字符串的开始处遍历到第 $i$ 个字符，这需要 $O(n)$ 的时间。
- **字符计数**: 与随机访问类似，计算 UTF-16 字符串的长度也是 $O(1)$ 的操作。但是，计算 UTF-8 编码的字符串的长度需要遍历整个字符串。
- **字符串操作**: 在 UTF-16 编码的字符串中，很多字符串操作（如分割、连接、插入、删除等）都更容易进行。在 UTF-8 编码的字符串上进行这些操作通常需要额外的计算，以确保不会产生无效的 UTF-8 编码。

编程语言的字符编码方案设计是一个很有趣的话题，涉及到许多因素：

- Java 的 `String` 类型使用 UTF-16 编码，每个字符占用 2 字节。这是因为 Java 语言设计之初，人们认为 16 位足以表示所有可能的字符。然而，这是一个不正确的判断。后来 Unicode 规范扩展到了超过 16 位，所以 Java 中的字符现在可能由一对 16 位的值（称为“代理对”）表示。
- JavaScript 和 TypeScript 的字符串使用 UTF-16 编码的原因与 Java 类似。当 JavaScript 语言在 1995 年被 Netscape 公司首次引入时，Unicode 还处于相对早期的阶段，那时候使用 16 位的编码就足够表示所有的 Unicode 字符了。
- C# 使用 UTF-16 编码，主要因为 .NET 平台是由 Microsoft 设计的，而 Microsoft 的很多技术，包括 Windows 操作系统，都广泛地使用 UTF-16 编码。

由于以上编程语言对字符数量的低估，它们不得不采取“代理对”的方式来表示超过 16 位长度的 Unicode 字符。这是一个不得已为之的无奈之举。一方面，包含代理对的字符串中，一个字符可能占用 2 字节或 4 字节，因此丧失了等长编码的优势。另一方面，处理代理对需要增加额外代码，这增加了编程的复杂性和 Debug 难度。

出于以上原因，部分编程语言提出了不同的编码方案：

- Python 3 使用一种灵活的字符串表示，存储的字符长度取决于字符串中最大的 Unicode 码点。对于全部是 ASCII 字符的字符串，每个字符占用 1 个字节；如果字符串中包含的字符超出了 ASCII 范围，但全部在基本多语言平面（BMP）内，每个字符占用 2 个字节；如果字符串中有超出 BMP 的字符，那么每个字符占用 4 个字节。
- Go 语言的 `string` 类型在内部使用 UTF-8 编码。Go 语言还提供了 `rune` 类型，它用于表示单个 Unicode 码点。
- Rust 语言的 str 和 String 类型在内部使用 UTF-8 编码。Rust 也提供了 char 类型，用于表示单个 Unicode 码点。

需要注意的是，以上讨论的都是字符串在编程语言中的存储方式，**这和字符串如何在文件中存储或在网络中传输是两个不同的问题**。在文件存储或网络传输中，我们一般会将字符串编码为 UTF-8 格式，以达到最优的兼容性和空间效率。